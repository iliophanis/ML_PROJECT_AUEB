{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab3 - Machine Learning\n",
    "## Multiclass Classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the times we encounter problems that are not binary. E.x. imagine you're given a photo and your task is to predict if it contains one of the following categories:\n",
    "    - dog,\n",
    "    - cat\n",
    "    - horse\n",
    "    - duck\n",
    "    - etc etc\n",
    "Given $K$ classes, you would like to assign $1$ class per data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Multi-class classification for Logistic Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given $K$ klasses a straightforward way is to train $K$ binary classifiers.\n",
    "    - One - Versus all\n",
    "    - One class belongs to the positive category, the rest belongs to the negative category\n",
    "- Then given a new data point $x$, we predict that the data point $x$ belongs to the class $i=\\{1,2,...,k\\}$ based on which classifier outputted the highest probability.\n",
    "\n",
    "![title](img/onevsall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Multi-class classification with Neural Networks </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Neural Networks we \"force\" the output of the $K$ units to be a discrete probability distribution over $K$ classes.\n",
    "\n",
    "\n",
    "A probability distribution is valid if and only if the values of the output units:\n",
    "\n",
    "1) must be non-negative\n",
    "\n",
    "2) must sum to 1\n",
    "\n",
    "\n",
    "\n",
    "$z \\in \\mathcal{R}^{Kx1}:$ the feature vector of the neural network(final output).\n",
    "\n",
    "We can convert $z$ to probabilities with the softmax function\n",
    "\n",
    "$$softmax(z) = \\frac{ e^{z} }{ \\sum_{i=1}^{k} e^{ z_{i} } } = \\{  \\frac{ e^{z_1} }{ \\sum_{i=1}^{k} e^{ z_{i} } }, \\frac{ e^{z_2} }{ \\sum_{i=1}^{k} e^{ z_{i} } }, \\dots, \\frac{ e^{z_K} }{ \\sum_{i=1}^{k} e^{ z_{i} } } \\} $$\n",
    "\n",
    "- Each unit is exponetiated\n",
    "- exp is always positive\n",
    "- The we normalize each unit to 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> One hot vectors </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A binary vector of length K, where each value is 0 except for the class it belongs, the value is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$t_{nk} \\in \\{ 0,1 \\}, \\sum_{k=1}^{K} t_{nk} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of k=3 classes\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    0\\\\\n",
    "    0\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "    0\\\\\n",
    "    1\\\\\n",
    "    0\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "    0\\\\\n",
    "    0\\\\\n",
    "    1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood\n",
    "$$p( T | W ) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} y_{nk}^{t_{nk}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function (logLikelihood plus reguralization term) we want to maximize for the problem of classifying N number of data in K categories/classes is:\n",
    "\n",
    "$$\n",
    "E(W) = \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\log y_{nk}   -  \\frac{\\lambda}{2} \\sum_{k=1}^K ||\\mathbf{w_k}||^2, \n",
    "$$\n",
    "\n",
    "where $y_{nk}$ is the softmax function defined as:\n",
    "\n",
    "$$y_{nk} = \\frac{e^{\\mathbf{w}_k^T \\mathbf{x}_n}}{\\sum_{j=1}^K e^{\\mathbf{w}_j^T \\mathbf{x}_n}}$$\n",
    "$W$ is a $K \\times (D+1)$ matrix where each line represents the vector $\\mathbf{w}_k$.\n",
    "\n",
    "\n",
    "The cost function can be simplified in the following form:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "E(W) = \\sum_{n=1}^N \\left[ \\left( \\sum_{k=1}^K t_{nk} \\mathbf{w}_k^T \\mathbf{x}_n \\right) - \\log \\left( \\sum_{j=1}^K e^{\\mathbf{w}_j^T \\mathbf{x}_n} \\right) \\right]   -  \\frac{\\lambda}{2} \\sum_{k=1}^K ||\\mathbf{w}_k||^2, \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "In the above formula we have used the fact that $\\sum_{k=1}^K t_{nk} = 1$. \n",
    "\n",
    "The partial derrivatives of this function are given by the following $K \\times (D+1)$ matrix:\n",
    "\n",
    "$$\n",
    "(T - S)^Î¤ X - \\lambda W,\n",
    "$$\n",
    "\n",
    "where $T$ is an $N \\times K$ matrix with the truth values of the training data, such that $[T]_{nk} = t_{nk}$, $S$ is the corresponding $N \\times K$ matrix that holds the softmax probabilities such that $[S]_{nk} = y_{nk}$ and $X$ is the $N \\times (D + 1)$ matrix of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Softmax </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use by default ax=1, when the array is 2D\n",
    "#use ax=0 when the array is 1D\n",
    "def softmax( x, ax=1 ):\n",
    "    m = np.max( x, axis=ax, keepdims=True )#max per row\n",
    "    p = np.exp( x - m )\n",
    "    return ( p / np.sum(p,axis=ax,keepdims=True) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array( [ [10,20,30,40], [20,50,45,45], [983,39,57,752], [574,575,597,525] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10,  20,  30,  40],\n",
       "       [ 20,  50,  45,  45],\n",
       "       [983,  39,  57, 752],\n",
       "       [574, 575, 597, 525]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.357198e-14</td>\n",
       "      <td>2.061060e-09</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>9.999546e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.233197e-14</td>\n",
       "      <td>9.867033e-01</td>\n",
       "      <td>0.006648</td>\n",
       "      <td>6.648354e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.764032e-101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.026188e-10</td>\n",
       "      <td>2.789468e-10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.380186e-32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1         2              3\n",
       "0  9.357198e-14  2.061060e-09  0.000045   9.999546e-01\n",
       "1  9.233197e-14  9.867033e-01  0.006648   6.648354e-03\n",
       "2  1.000000e+00  0.000000e+00  0.000000  4.764032e-101\n",
       "3  1.026188e-10  2.789468e-10  1.000000   5.380186e-32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame( softmax( z ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D case\n",
    "\n",
    "Let's take an example where all values are equal. As expected everything is 1 / |z|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array( [1,1,1,1] )\n",
    "softmax( z, ax=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Mnist Dataset </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data folder there is the dataset of mnist. Mnists consists of $28x28$ grayscale images. In total there are 10 training files\n",
    "train0.txt, train1.txt, ..., train9.txt where each rows of train$k$.txt corresponds to an example that belongs to the class $k$.\n",
    "\n",
    "The testing data follows the same format.\n",
    "\n",
    "In total we have $6*10^5$ training examples and $10^3$ testing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the MNIST dataset. Reads the training and testing files and create matrices.\n",
    "    :Expected return:\n",
    "    train_data:the matrix with the training data\n",
    "    test_data: the matrix with the data that will be used for testing\n",
    "    y_train: the matrix consisting of one \n",
    "                        hot vectors on each row(ground truth for training)\n",
    "    y_test: the matrix consisting of one\n",
    "                        hot vectors on each row(ground truth for testing)\n",
    "    \"\"\"\n",
    "    \n",
    "    #load the train files\n",
    "    df = None\n",
    "    \n",
    "    y_train = []\n",
    "\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'data/mnist/train%d.txt' % i, header=None, sep=\" \" )\n",
    "        #build labels - one hot vector\n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_train.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    train_data = df.to_numpy()\n",
    "    y_train = np.array( y_train )\n",
    "    \n",
    "    #load test files\n",
    "    df = None\n",
    "    \n",
    "    y_test = []\n",
    "\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'data/mnist/test%d.txt' % i, header=None, sep=\" \" )\n",
    "        #build labels - one hot vector\n",
    "        \n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_test.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    test_data = df.to_numpy()\n",
    "    y_test = np.array( y_test )\n",
    "    print(train_data[:5])\n",
    "    return train_data, test_data, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/mnist/train0.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m load_data()\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(X_train\u001b[39m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m y_train \u001b[39m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m( \u001b[39m10\u001b[39m ):\n\u001b[1;32m---> 19\u001b[0m     tmp \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv( \u001b[39m'\u001b[39;49m\u001b[39mdata/mnist/train\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m.txt\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m i, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m )\n\u001b[0;32m     20\u001b[0m     \u001b[39m#build labels - one hot vector\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     hot_vector \u001b[39m=\u001b[39m [ \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m j \u001b[39m==\u001b[39m i \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m10\u001b[39m) ]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/mnist/train0.txt'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> View of the dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 5 random images from the training set\n",
    "n = 100\n",
    "sqrt_n = int( n**0.5 )\n",
    "samples = np.random.randint(X_train.shape[0], size=n)\n",
    "\n",
    "plt.figure( figsize=(11,11) )\n",
    "\n",
    "cnt = 0\n",
    "for i in samples:\n",
    "    cnt += 1\n",
    "    plt.subplot( sqrt_n, sqrt_n, cnt )\n",
    "    plt.subplot( sqrt_n, sqrt_n, cnt ).axis('off')\n",
    "    plt.imshow( X_train[i].reshape(28,28), cmap='gray'  )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Normalize the dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(float)/255\n",
    "X_test = X_test.astype(float)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add column of ones to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack( (np.ones((X_train.shape[0],1) ), X_train) )\n",
    "X_test = np.hstack( (np.ones((X_test.shape[0],1) ), X_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cost function\n",
    "\n",
    "$$E(w)=\\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\log( y_{nk} )-  \\frac{\\lambda}{2} ||\\mathbf{w}||^2, \n",
    "$$\n",
    "\n",
    "$$\n",
    "E(w) = \\sum_{n=1}^N \\left[ \\left( \\sum_{k=1}^K t_{nk} \\mathbf{w}_k^T \\mathbf{x}_n \\right) - \\log \\left( \\sum_{j=1}^K e^{\\mathbf{w}_j^T \\mathbf{x}_n} \\right) \\right]   -  \\frac{\\lambda}{2} ||\\mathbf{w}||^2, \n",
    "$$\n",
    "\n",
    "$$ \\frac{ \\partial E(w) }{ \\partial w } = (T-Y)^{T} X - \\lambda w$$\n",
    "\n",
    "\n",
    "we can also use the logsumexp trick, where m is the maximum element for numerical stability\n",
    "\\begin{align} \n",
    "\\log \\sum_{i=1}^{n} e^{\\mathbf{w}_j^T \\mathbf{x}_n} &= \\log \\Bigr( \\sum_{i=1}^{n} e^{\\mathbf{w}_j^T \\mathbf{x}_n +m -m}\\Bigl) \\\\ \n",
    "&= \\log \\Bigr( \\sum_{i=1}^{n} e^m e^{\\mathbf{w}_j^T \\mathbf{x}_n-m} ) \\Bigl) \\\\ \n",
    "&= \\log \\Bigr( e^m \\sum_{i=1}^{n} e^{\\mathbf{w}_j^T \\mathbf{x}_n-m} ) \\Bigl) \\\\ \n",
    "&= \\log \\ e^m + \\log \\Bigr( \\sum_{i=1}^{n} e^{\\mathbf{w}_j^T \\mathbf{x}_n-m} ) \\Bigl) \\\\ \n",
    "&= m + \\log \\Bigr( \\sum_{i=1} e^{\\mathbf{w}_j^T \\mathbf{x}_n-m}  \\Bigl) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Winit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = X_train.shape\n",
    "K = 10\n",
    "Winit = 0.5 * np.ones((K, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost_grad_softmax(W, X, t, lamda):\n",
    "    \n",
    "    #X: NxD\n",
    "    #W: KxD\n",
    "    #t: NxD\n",
    "    \n",
    "    E = 0\n",
    "    N, D = X.shape\n",
    "    K = t.shape[1]\n",
    "    gradEW = np.zeros_like( (K,D) )\n",
    "    \n",
    "    return E, gradEw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_grad_softmax(Winit,X_train,y_train,0.125)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output\n",
    "\n",
    "(-138277.60557972503,\n",
    " array([[-0.0625, -0.0625, -0.0625, ..., -0.0625, -0.0625, -0.0625],\n",
    "        [-0.0625, -0.0625, -0.0625, ..., -0.0625, -0.0625, -0.0625],\n",
    "        [-0.0625, -0.0625, -0.0625, ..., -0.0625, -0.0625, -0.0625],\n",
    "        ...,\n",
    "        [-0.0625, -0.0625, -0.0625, ..., -0.0625, -0.0625, -0.0625],\n",
    "        [-0.0625, -0.0625, -0.0625, ..., -0.0625, -0.0625, -0.0625],\n",
    "        [-0.0625, -0.0625, -0.0625, ..., -0.0625, -0.0625, -0.0625]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ml_softmax_train(t, X, lamda, Winit, options):\n",
    "    \"\"\"inputs :\n",
    "      t: N x 1 binary output data vector indicating the two classes\n",
    "      X: N x (D+1) input data vector with ones already added in the first column\n",
    "      lamda: the positive regularizarion parameter\n",
    "      winit: D+1 dimensional vector of the initial values of the parameters\n",
    "      options: options(1) is the maximum number of iterations\n",
    "               options(2) is the tolerance\n",
    "               options(3) is the learning rate eta\n",
    "    outputs :\n",
    "      w: the trained D+1 dimensional vector of the parameters\"\"\"\n",
    "\n",
    "    W = Winit\n",
    "\n",
    "    # Maximum number of iteration of gradient ascend\n",
    "    _iter = options[0]\n",
    "\n",
    "    # Tolerance\n",
    "    tol = options[1]\n",
    "\n",
    "    # Learning rate\n",
    "    eta = options[2]\n",
    "\n",
    "    Ewold = -np.inf\n",
    "    costs = []\n",
    "    for i in range( 1, _iter+1 ):\n",
    "        Ew, gradEw = cost_grad_softmax(W, X, t, lamda)\n",
    "        # save cost\n",
    "        costs.append(Ew)\n",
    "        # Show the current cost function on screen\n",
    "        if i % 50 == 0:\n",
    "            print('Iteration : %d, Cost function :%f' % (i, Ew))\n",
    "\n",
    "        # Break if you achieve the desired accuracy in the cost function\n",
    "        if np.abs(Ew - Ewold) < tol:\n",
    "            break\n",
    "                \n",
    "        # Update parameters based on gradient ascend\n",
    "        W = W + eta * gradEw\n",
    "\n",
    "        Ewold = Ew\n",
    "\n",
    "    return W, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient checking\n",
    "\n",
    "\n",
    "During gradient ascent/descent we compute the gradients $\\frac{\\partial E}{\\partial w}$, where $w$ denotes the parameters of the model.\n",
    "\n",
    "In order to make sure that these gradients are correct we will compare the exact gradients(that we have coded) with numerical estimates obtained by finite differences, you can use your code for computing $E$ to verify the code for computing $\\frac{\\partial E}{\\partial w}$.\n",
    "    Let's look back at the definition of a derivative (or gradient):\n",
    "    \n",
    "$$ \\frac{\\partial E}{\\partial w} = \\lim_{\\varepsilon \\to 0} \\frac{E(w + \\varepsilon) - E(w - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$  \n",
    "\n",
    "We know the following: \n",
    "- $\\frac{\\partial E}{\\partial w}$ is what you want to make sure you're computing correctly. ,\n",
    "- You can compute $E(w + \\varepsilon)$ and $E(w - \\varepsilon)$ (in the case that $w$ is a real number), since you're confident your implementation for $E$ is correct.\n",
    "\n",
    "Let's use equation (1) and a small value ( around $10^-4$ or $10^-6$, much smaller values could lead to numerical issues )for $\\varepsilon$ to make sure that your code for computing  $\\frac{\\partial E}{\\partial w}$ is correct!\n",
    "\n",
    "![title](img/grad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradcheck_softmax(Winit, X, t, lamda):\n",
    "    \n",
    "    W = np.random.rand(*Winit.shape)\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    _list = np.random.randint(X.shape[0], size=5)\n",
    "    x_sample = np.array(X[_list, :])\n",
    "    t_sample = np.array(t[_list, :])\n",
    "    \n",
    "    Ew, gradEw = cost_grad_softmax(W, x_sample, t_sample, lamda)\n",
    "    \n",
    "    print( \"gradEw shape: \", gradEw.shape )\n",
    "    \n",
    "    numericalGrad = np.zeros(gradEw.shape)\n",
    "    # Compute all numerical gradient estimates and store them in\n",
    "    # the matrix numericalGrad\n",
    "    for k in range(numericalGrad.shape[0]):\n",
    "        for d in range(numericalGrad.shape[1]):\n",
    "            \n",
    "            #add epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W)\n",
    "            w_tmp[k, d] += epsilon\n",
    "            e_plus, _ = cost_grad_softmax(w_tmp, x_sample, t_sample, lamda)\n",
    "\n",
    "            #subtract epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W)\n",
    "            w_tmp[k, d] -= epsilon\n",
    "            e_minus, _ = cost_grad_softmax( w_tmp, x_sample, t_sample, lamda)\n",
    "            \n",
    "            #approximate gradient ( E[ w[k,d] + theta ] - E[ w[k,d] - theta ] ) / 2*e\n",
    "            numericalGrad[k, d] = (e_plus - e_minus) / (2 * epsilon)\n",
    "    \n",
    "    return ( gradEw, numericalGrad )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = X_train.shape\n",
    "\n",
    "K = 10#num of classes\n",
    "\n",
    "# initialize w for the gradient ascent\n",
    "Winit = np.zeros((K, D))\n",
    "\n",
    "# regularization parameter\n",
    "lamda = 0.1\n",
    "\n",
    "# options for gradient descent\n",
    "options = [500, 1e-6, 0.5/N]# Maximum number of iteration of gradient ascend, Tolerance, Learning rate\n",
    "\n",
    "gradEw, numericalGrad = gradcheck_softmax(Winit, X_train, y_train, lamda)\n",
    "\n",
    "# Absolute norm\n",
    "print( \"The difference estimate for gradient of w is : \", np.max(np.abs(gradEw - numericalGrad)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the first rows of the gradient and of the gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gradient values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame( gradEw )#.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The approximations of the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame( numericalGrad ).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# N of X\n",
    "N, D = X_train.shape\n",
    "\n",
    "K = 10\n",
    "\n",
    "# initialize w for the gradient ascent\n",
    "Winit = np.zeros((K, D))\n",
    "\n",
    "# regularization parameter\n",
    "lamda = 0.1\n",
    "\n",
    "# options for gradient descent\n",
    "options = [500, 1e-6, 0.5/N]\n",
    "\n",
    "#gradcheck_softmax(Winit, X_train, y_train, lamda)\n",
    "\n",
    "# Train the model\n",
    "W, costs = ml_softmax_train(y_train, X_train, lamda, Winit, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(format(options[2], 'f')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Predict </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ml_softmax_test(W, X_test):\n",
    "    ytest = softmax( X_test.dot(W.T) )\n",
    "    # Hard classification decisions\n",
    "    ttest = np.argmax(ytest, 1)\n",
    "    return ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = ml_softmax_test(W, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Accuracy </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean( pred == np.argmax(y_test,1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at our test data. Check out some of the misclassified test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "faults = np.where(np.not_equal(np.argmax(y_test,1),pred))[0]\n",
    "# plot n misclassified examples from the Test set\n",
    "n = 25\n",
    "samples = np.random.choice(faults, n)\n",
    "sqrt_n = int( n ** 0.5 )\n",
    "\n",
    "plt.figure( figsize=(11,13) )\n",
    "\n",
    "cnt = 0\n",
    "for i in samples:\n",
    "    cnt += 1\n",
    "    plt.subplot( sqrt_n, sqrt_n, cnt )\n",
    "    plt.subplot( sqrt_n, sqrt_n, cnt ).axis('off')\n",
    "    plt.imshow( X_test[i,1:].reshape(28,28)*255, cmap='gray' )\n",
    "    plt.title(\"True: \"+str(np.argmax(y_test,1)[i])+ \"\\n Predicted: \"+ str(pred[i]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is in the <strong>W</strong> weight array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(11,13) )\n",
    "cnt = 0\n",
    "for i in np.delete( W, 0, axis=1 ):\n",
    "    cnt+=1\n",
    "    plt.subplot( 1, 10, cnt ).axis('off')\n",
    "    plt.title( cnt-1 )\n",
    "    plt.imshow( i.reshape( (28,28) ).reshape(28,28)*255, cmap='gray' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras - Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras is TensorFlow's high-level API for building and training deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The needed version of running this experiment was tensorflow 1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's remove the columns of 1 we had inserted in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help( np.delete )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis=1 refers that it will delete columns, meanwhile axis=0 would refer to rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.delete( X_train, 0, axis=1 ).shape#delete first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.delete( X_train, 0, axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.delete( X_test, 0, axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we just check image format on how to prepare our data. It could be either\n",
    "\n",
    "(img_rows,img_cols,img_channels)\n",
    "or\n",
    "(img_channels,img_rows,img_cols)mn\n",
    "\n",
    "In our case the channel will go to the last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( tf.keras.backend.image_data_format() )\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "#channels = number of colors\n",
    "if tf.keras.backend.image_data_format() == 'channels_last':\n",
    "    X_train = X_train.reshape( (-1,img_rows,img_cols,1) )\n",
    "    X_test = X_test.reshape( (-1,img_rows,img_cols,1) )\n",
    "    input_shape = ( img_rows, img_cols, 1 )\n",
    "else:\n",
    "    X_train = X_train.reshape( (-1,1,img_rows,img_cols) )\n",
    "    X_test = X_test.reshape( (-1,1,img_rows,img_cols) )\n",
    "    input_shape = ( 1, img_rows, img_cols )\n",
    "num_classes = 10\n",
    "print( input_shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add( Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add( Conv2D(64, (3, 3), activation='relu') )\n",
    "model.add( MaxPooling2D(pool_size=(2, 2)) )\n",
    "model.add( Conv2D(64, (3, 3), activation='relu') )\n",
    "model.add( MaxPooling2D(pool_size=(2, 2)) )\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile( loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_log = model.fit( X_train, y_train, validation_data=(X_test,y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot( model_log.history['acc'])\n",
    "plt.plot( model_log.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(model_log.history['loss'])\n",
    "plt.plot( model_log.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Activations of Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we gather all the layers's output and we construct a model that given an input, it will return all of the activations back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [ layer.output for layer in model.layers ]\n",
    "activation_model = tf.keras.Model( inputs=model.input, outputs=layer_outputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [ layer.name for layer in model.layers ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = X_train[0].reshape( (-1,28,28,1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image we will feed into the network will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( img.reshape((28,28)), cmap='gray' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's get the activations values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = activation_model.predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of the shapes in the form of (1,img_rows,img_cols,num_filters), so what is left to do, is to iterate through all num_filters and plot the\n",
    "\n",
    "img_rows $\\times$ img_cols images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, lname in zip( activations, layer_names ):\n",
    "    if len(i.shape) == 4:\n",
    "        fig = plt.figure( figsize=(11,11) )\n",
    "        ax = fig.add_subplot(9,16,1)\n",
    "        ax.title.set_text( lname )\n",
    "        num = i.shape[-1]\n",
    "        for k in range( num ):\n",
    "            tmp = i[:,:,:,k]\n",
    "            tmp = tmp.reshape( (tmp.shape[1], tmp.shape[2]) )\n",
    "            ax = fig.add_subplot(9,16,k+1)\n",
    "            ax.axis('off')\n",
    "            ax.imshow( tmp )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
